---
title: "Improving Model Perfromance / Tuning Parameters"
author: "Michael Raminski"
date: "`r Sys.Date()`"
output: html_document
---


## Tuning Parameter

Generically and regardless of model type, what are the purposes of a model
tuning parameters? 

The general purpose of tuning a model is to optimize the modeling process. This can pertain to:
  -Model performance
  -Model fit
  -Parameter selection
  -Variance-bias tradeoff


####Load packages
```{r, warning=FALSE, message=FALSE, error=FALSE, eval=TRUE, include=FALSE}
library(readr)
library(dplyr)
library(plyr)
library(tibble)
library(Hmisc)
library(pastecs)
library(reshape2)
library(ggplot2)
library(ggm)
library(corrgram)
library(psych)
library(car)
library(gvlma)
library(qcc)
library(psych)
library(rpart)
library(rpart.plot)
library(caret)
library(pROC)
library(cluster)
library(StatMatch)
library(kknn)
library(flexclust)
library(clue)
library(NbClust)
library(C50)
library(fastAdaboost)
library(randomForest)
library(class)
```

####Import data
```{r, warning=FALSE, message=FALSE, error=FALSE, eval=TRUE, include=FALSE}
delays <- read.csv("all.csv")
#delays <- tbl_df(delays)
delays$delay15[delays$arr_delay >= 15] <- "Pos"
delays$delay15[delays$arr_delay < 15] <- "Neg"
delays <- select(delays, c(2,4,6,10,15,16,20,22,25,27,45,50))
delaysclean <- na.omit(delays)
delaysclean

set.seed(9999)
inTraining <- caret::createDataPartition(delaysclean$month, p = .01, list = FALSE)
training <- delaysclean[ inTraining,]
testing  <- delaysclean[-inTraining,]
training
testing
```

## Caret Models

This assignment demonstrates the use of caret for constructing models. Each
model should be built and compared using using `Kappa` as the performance
metric calculated using 10-fold repeated cross-validation with 3 folds.

Using the rectangular data that you created for the NYCFlights to create a model
for arr_delay >= 15 minutes.

- glm
- rpart
- knn
- C50
- randomForest
- adaBoost
- Two methods of your choice from the Caret Model List (you will need to install any dependencies)

Save the caret objects with the names provided.


```{r, warning=FALSE, message=FALSE, error=FALSE, eval=TRUE}
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, classProbs = TRUE)

fit.glm <- train(delay15 ~ ., data = training, method = "glm", metric = "Kappa", trControl = ctrl)
fit.glm
fit.rpart <- train(delay15 ~ ., data = training, method = "rpart", metric = "Kappa", trControl = ctrl, tuneLength = 50)
fit.rpart
fit.knn <- train(delay15 ~ ., data = training, method = "knn", metric = "Kappa", trControl = ctrl, tuneLength = 5)
fit.knn
fit.c50 <- train(delay15 ~ ., data = training, method = "C5.0Tree", metric = "Kappa", trControl = ctrl)
fit.c50
#Calculation too slow for Random Forest and adaBoost
#fit.rf <- train(delay15 ~ ., data = training, method = "rf", metric = "Kappa", trControl = ctrl)
#fit.rf
#fit.adaB <- train(delay15 ~ ., data = training, method = "adaboost", metric = "Kappa", trControl = ctrl)
#fit.adaB
fit.myown1 <- train(delay15 ~ ., data = training, method = "lvq", metric = "Kappa", trControl = ctrl)
fit.myown1
fit.myown2 <- train(delay15 ~ ., data = training, method = "rpart2", metric = "Kappa", trControl = ctrl)
fit.myown2
```




Compare the  models?

Which is best?  Why?

Comparing the models based solely on the Kappa metric, makes glm the best model (values shown below). However, it should be noted that because of the burden of the calculation for several of the models, I used only 1% of the data for my training set.

Kappa:
glm: .7149
rpart (Cp = .5064): .6532
knn (k=5): .3807
c50: .6475
rf (mtry = 25): .6587
adaBoost: (could not run)
lvq (size = 22, k=11): .1585
rpart2 (maxdepth =1): .6520







